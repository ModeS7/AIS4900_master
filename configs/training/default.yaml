# Default Training Configuration
#
# Common training hyperparameters used across all training scripts.
# Override via CLI: python -m medgen.scripts.train training.epochs=1000

# Experiment name prefix for run directory
# Include trailing underscore: training.name=exp1_ -> exp1_rflow_128_...
name: ""

# Core training parameters
epochs: 500                # Total training epochs
batch_size: 16             # Batch size per GPU
limit_train_batches: null  # Limit batches per epoch (null = use all, int = limit)
learning_rate: 1.0e-4      # Base learning rate
gradient_clip_norm: 1.0    # Max gradient norm (prevents explosions)
warmup_epochs: 5           # Linear warmup epochs (0.1x -> 1x LR)
figure_count: 20           # Target number of figure logs per training run (adaptive interval)
augment: true              # Data augmentation (horizontal flip, small rotation)
augment_type: diffusion    # Augmentation type: "diffusion" (conservative) or "vae" (aggressive)

# Batch-level augmentations (VAE only - mixup/cutmix)
batch_augment:
  enabled: false           # Disabled by default, enabled in VAE configs
  mixup_prob: 0.0          # Probability of mixup per batch
  cutmix_prob: 0.0         # Probability of cutmix per batch

# Score Augmentation - applies transforms to noisy data (after noise addition)
# Reference: https://arxiv.org/abs/2508.07926
# Unlike traditional augmentation (which augments clean data), ScoreAug transforms
# the noisy input and target together, teaching equivariant denoising.
#
# Per paper: "one augmentation method is randomly selected with equal probability
# across all types" - identity is always included, so with 4 transforms enabled,
# each has 20% probability (1/5).
#
# Conditioning requirements:
# - rotation/flip: REQUIRES omega conditioning (noise distribution is invariant)
# - translation/cutout: Work without conditioning but risk data leakage
# - brightness: Linear transform, works without conditioning
score_aug:
  enabled: false             # Disabled by default
  # Transforms - each enabled transform has equal probability
  # Spatial transforms (D4 dihedral symmetries) - rotation and flip are combined:
  #   - rotation only: 3 options (90°, 180°, 270°)
  #   - flip only: 2 options (hflip, vflip)
  #   - both: 7 options (3 rot + 2 flip + 2 rot+flip combos)
  rotation: true             # 90, 180, 270 degree rotations (requires omega)
  flip: true                 # Horizontal + vertical flip (requires omega)
  translation: false         # +-40% X, +-20% Y translation with zero-pad
  cutout: false              # Random rectangle cutout (10-30% each dimension)
  brightness: false          # Brightness scaling (experimental with normalized data)
  brightness_range: 1.2      # Max scale factor B, samples in [1/B, B]
  use_omega_conditioning: true   # Always use omega conditioning when ScoreAug is enabled
  # Compose mode: apply transforms independently instead of picking one
  compose: false             # If true, each transform applied with compose_prob
  compose_prob: 0.5          # Probability for each transform in compose mode

  # === v2 mode: structured non-destructive/destructive augmentation ===
  # When enabled, separates transforms into:
  # - Non-destructive (can stack): rotation, flip, translation
  # - Destructive (pick one): cutout OR fixed patterns
  v2_mode: false             # Enable v2 structured augmentation
  nondestructive_prob: 0.5   # Probability for each non-destructive transform
  destructive_prob: 0.5      # Probability of applying any destructive transform
  cutout_vs_pattern: 0.5     # Split between cutout (random) and patterns (fixed)

  # Fixed pattern options (16 total patterns, 4 per category)
  # Patterns are deterministic masks learned via one-hot embedding
  patterns:
    checkerboard: true       # 4×4 and 8×8 alternating grids (IDs 0-3)
    grid_dropout: true       # Random 25%/50% grid cells dropped (IDs 4-7)
    coarse_dropout: true     # 2-4 large holes at corners/edges (IDs 8-11)
    patch_dropout: true      # MAE-style 25%/50% patches dropped (IDs 12-15)

# DataLoader optimization for CPU augmentation
dataloader:
  num_workers: 8           # Parallel data loading workers
  prefetch_factor: 4       # Batches to prefetch per worker
  pin_memory: true         # Faster CPU->GPU transfer
  persistent_workers: true # Avoid worker respawn overhead

# EMA (Exponential Moving Average)
# Maintains slowly-updated weight copy for higher quality samples
use_ema: false
ema:
  decay: 0.9999            # EMA decay rate (0.999-0.9999)
  update_after_step: 100   # Start EMA after this many steps
  update_every: 10         # Update EMA every N steps

# Min-SNR Loss Weighting
# Reweights per-sample loss by timestep to prevent high-noise step domination
# Reference: https://arxiv.org/abs/2303.09556
use_min_snr: false
min_snr_gamma: 5.0         # SNR clipping threshold

# SAM (Sharpness-Aware Minimization)
# Seeks flat minima for better generalization. Requires 2x compute per step.
# Reference: https://arxiv.org/abs/2010.01412
sam:
  enabled: false             # Enable SAM optimizer
  rho: 0.05                  # Perturbation radius (0.01-0.1 typical)
  adaptive: false            # Use ASAM (adaptive, weight-scale invariant)

# Perceptual loss (SqueezeNet features)
perceptual_weight: 0.001   # Weight for perceptual loss term

# Multi-GPU (DDP)
use_multi_gpu: false
disable_ddp_optimizer: false  # Disable DDP optimizer for large models (saves memory)

# Learning rate scheduler
eta_min: 1.0e-6              # Minimum LR for cosine annealing

# torch.compile optimization
# Compiles model and loss functions for faster training
use_compile: true            # Compile VAE/discriminator/perceptual loss
compile_fused_forward: true  # Compile fused forward pass (diffusion only, ~10% speedup)
                             # Auto-disabled for latent space or Min-SNR

# Precision settings (VAE only)
# Pure BF16 training stores model weights in BF16 for memory savings
# Autocast continues to work as usual - this only affects weight storage
precision:
  dtype: bf16                # bf16, fp16, fp32 - model weight dtype when pure_weights=true
  pure_weights: false        # If true, model weights stored in low precision (saves ~50% memory)

# Logging and metrics (all logged every epoch, figures at figure_interval)
# Mode compatibility:
#   [seg] = segmentation only
#   [cond] = bravo/dual only
#   [all] = all modes
logging:
  # Training dynamics
  grad_norm: true              # [all] Track gradient norm (catches instability)
  timestep_losses: true        # [all] Loss by diffusion timestep
  regional_losses: true        # [cond] Loss by tumor vs background region
  timestep_region_losses: true # [cond] 2D heatmap: timestep x region

  # Validation metrics
  msssim: true                 # [cond] Multi-Scale Structural Similarity (2D/3D)
  psnr: true                   # [cond] Peak signal-to-noise ratio
  lpips: true                  # [cond] Learned Perceptual (2D only, slower)
  boundary_sharpness: true     # [cond] Edge quality in tumor regions

  # Visualization
  intermediate_steps: true     # [cond] Save denoising trajectory
  worst_batch: true            # [all] Save batch with highest loss each epoch
  num_intermediate_steps: 5    # How many steps to visualize

  # Performance
  flops: true                  # [all] Measure model FLOPs (once at start)

# PyTorch Profiler - exports Chrome traces for performance analysis
# View traces: Perfetto UI (https://ui.perfetto.dev) or chrome://tracing
# Reference: https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html
profiling:
  enabled: false              # Master switch
  wait: 5                     # Steps to skip (torch.compile warmup)
  warmup: 2                   # Steps to warm up profiler (discarded)
  active: 10                  # Steps to actively profile
  repeat: 1                   # Number of profiling cycles (0 = continuous)
  record_shapes: true         # Record tensor shapes
  profile_memory: true        # Track memory allocations
  with_stack: false           # Include Python call stacks (slower)
  with_flops: true            # Estimate FLOPs per operation
