# 3D VAE Training Configuration
#
# Trains 3D AutoencoderKL for volumetric latent diffusion.
# Based on MONAI Generative proven configuration.
#
# Usage:
#   python -m medgen.scripts.train_vae_3d
#
#   # Pure VAE (no GAN)
#   python -m medgen.scripts.train_vae_3d vae_3d.adv_weight=0
#
#   # Cluster training
#   python -m medgen.scripts.train_vae_3d paths=cluster

defaults:
  - paths: local
  - model: default
  - mode: dual
  - vae_3d: default
  - training: default
  - _self_

# Pretrained checkpoint (for fine-tuning)
pretrained_checkpoint: null

# ============================================================================
# 3D Volume Configuration
# ============================================================================
volume:
  # Input dimensions (after padding)
  depth: 160                      # Pad 150 -> 160 for clean 8x compression
  height: 256
  width: 256
  # Padding
  pad_depth_to: 160               # Target depth after padding
  pad_mode: replicate             # replicate or constant (zeros)
  # Slice subsampling (for quick testing)
  slice_step: 1                   # 1=all slices, 2=every 2nd, 3=every 3rd

# ============================================================================
# Training Overrides for 3D
# ============================================================================
training:
  epochs: 125
  batch_size: 2                   # Small batch for 3D (memory)
  learning_rate: 5.0e-5           # Lower LR for 3D (MONAI recommendation)
  gradient_clip_norm: 1.0
  warmup_epochs: 5
  # num_validations: 20 from default.yaml (125 epochs / 20 = val every 6 epochs)
  use_ema: false                  # EMA doubles memory
  augment_type: vae_3d
  # torch.compile disabled - conflicts with gradient checkpointing
  # (torch.compile + torch.utils.checkpoint have compatibility issues)
  use_compile: false
  # Gradient checkpointing via CheckpointedAutoencoder wrapper (~25-40% memory reduction)
  # Allows 256x256x160 to fit on 80GB A100
  gradient_checkpointing: true
  # Logging (inherits most from default.yaml)
  logging:
    grad_norm: true
    regional_losses: true         # Tumor/background loss tracking
    msssim: true                  # 3D MS-SSIM via MONAI
    psnr: true
    lpips: true                   # Slice-by-slice LPIPS (160 slices ~6s)
    flops: true                   # FLOPs per sample tracking

hydra:
  run:
    dir: ${paths.model_dir}/compression_3d/${mode.name}/${oc.select:training.name,""}${volume.height}x${volume.depth}_${now:%Y%m%d-%H%M%S}
  sweep:
    dir: ${paths.model_dir}/compression_3d/multirun/${now:%Y%m%d-%H%M%S}
    subdir: ${hydra.job.num}
