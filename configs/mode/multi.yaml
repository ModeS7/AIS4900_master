# Multi Mode - Multi-modality DIFFUSION with mode embedding
#
# NOTE: This is different from multi_modality.yaml which is for VAE training.
# - multi.yaml: DIFFUSION with mode embedding (model learns which modality to generate)
# - multi_modality.yaml: VAE training (pools all modalities, no mode embedding)
# See also: ModeType.MULTI vs ModeType.MULTI_MODALITY in src/medgen/core/constants.py
#
# Trains on all modalities (bravo, flair, t1_pre, t1_gd) conditioned on seg mask.
# Each sample includes mode_id so the model knows which modality to generate.
#
# Architecture:
#   in_channels: 2 = [noisy_image, seg_mask] - same as bravo mode
#   out_channels: 1 = predicted noise/velocity
#   Extra: mode_id embedding added to time_embed
#
# Dataloader: create_multi_diffusion_dataloader()
# Returns: (image, seg, mode_id) tuples where mode_id is 0-3
#
# Mode Embedding Strategy:
#   - 'full': Standard mode embedding (default, original behavior)
#   - 'dropout': Randomly drop mode embedding with probability mode_embedding_dropout
#   - 'none': No mode embedding (hard parameter sharing, forces shared representations)
#   - 'late': Late conditioning (inject mode only in later UNet blocks)

name: multi
is_conditional: true
in_channels: 2   # [noisy_image, seg_mask] - same as bravo
out_channels: 1  # predicted noise/velocity
image_keys: [bravo, flair, t1_pre, t1_gd]
use_mode_embedding: true
mode_embedding_strategy: full  # Options: full, dropout, none, late
mode_embedding_dropout: 0.2    # Probability of dropping mode embedding (when strategy=dropout)
late_mode_start_level: 2       # UNet level to start injecting mode (when strategy=late, 0-indexed)
description: "Multi-modality diffusion with mode embedding (4x training data)"
