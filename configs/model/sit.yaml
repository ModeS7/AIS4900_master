# SiT (Scalable Interpolant Transformer) Configuration
#
# Transformer-based diffusion model designed for flow matching.
# Override via CLI: python -m medgen.scripts.train model=sit model.variant=L
#
# Variants:
#   S:  hidden_size=384,  depth=12, heads=6  (~33M params)
#   B:  hidden_size=768,  depth=12, heads=12 (~130M params)
#   L:  hidden_size=1024, depth=24, heads=16 (~458M params)
#   XL: hidden_size=1152, depth=28, heads=16 (~675M params)

type: sit                            # Model type (sit for transformer)
spatial_dims: 2                      # Spatial dimensions (2 or 3)
image_size: 128                      # Input image size
patch_size: 2                        # Patch size (2, 4, or 8)
variant: B                           # Model variant: S, B, L, XL
mlp_ratio: 4.0                       # MLP expansion ratio
conditioning: concat                 # Conditioning mode: concat or cross_attn
drop_rate: 0.0                       # Dropout rate (applied within attention/MLP)

# ============================================================================
# Stochastic Depth (DropPath)
# ============================================================================
# Randomly drops entire transformer blocks during training.
# Rate increases linearly from 0 (first block) to drop_path_rate (last block).
# Recommended values:
#   - Small datasets (<10K): 0.1 - 0.2
#   - Medium datasets (10-50K): 0.05 - 0.1
#   - Large datasets (>50K): 0.0 - 0.05
drop_path_rate: 0.0                  # Stochastic depth rate (0.0 = disabled)

# ============================================================================
# QK-Normalization
# ============================================================================
# Applies LayerNorm to query and key vectors before attention computation.
# Prevents attention score explosion with high-variance inputs.
# Critical for training stability with larger images/patches.
# Reference: https://arxiv.org/abs/2302.05442 (Scaling Vision Transformers)
qk_norm: true                        # QK-normalization (recommended: true)
